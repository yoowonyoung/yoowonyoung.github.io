---
layout: post
title: "Kubernetes"
description: Kubernetes 공부
date: 2021-01-14 15:12:00 +09:00
categories: Kubernetes Study
permalink: '/Kubernetes'
---

# 쿠버네티스 클러스터 아키텍처
- 쿠버네티스의 클러스터는 하드웨어 수준에서 많은 노드로 구성되며 2가지 유형으로 나뉨
    * 마스터 노드: 전체 쿠버네티스 시스템을 관리하고 통제하는 쿠버네티스 컨트롤 플레인을 관장
    * 워커 노트: 실제 배포하고자 하는 애플리케이션 실행을 담당
    * 마스터 노드에서 워커 노드 없이 가능 하지만, 보통은 나눠서 함

- 마스터의 컨트롤 플레인 안에는 명령을 내리는 kube-apiserver가 있고, 워커노드에는 명령을 받는 kubelet이 있음
    * 컨트롤 플레인은 클러스터를 관리하는 기능, 단일 마스터에서 돌아가기도 하고, 여러 마스터 노드에 분할되고 복제되어 고가용성을 보장. 클러스터의 상태를 유지하고 제어하지만 애플리케이션을 실행하지는 않음(애플리케이션도 실행은 가능하지만, 일반적인 경우는 아님)
    * 워커노드는 컨테이너화된 애플리케이션을 실행하는 시스템
    * kube-apiserver는 사용자 인증같은걸 담당하기도 하고, 마스터 노드 내부의 통신 및 마스터노드와 워커노드의 kubelet과의 통신을 담당. 사실상 컨트롤 타워
    * kube-apiserver의 데이터베이스는 etcd를 씀
    * kube-scheduler와 control-manager 도 있음. 각각 스케줄링 알고리즘(어느 노드에 뭘 배치할지)이나 관련 리소스(yaml 파일과 같은 리소스등) 담당하고 있음
    * 사실상 kube-apiserver는 일을 그렇게 하는것이 아님. 통신을 담당하기 때문
    * kubelet은 kube-apiserver의 명령을 받아서 컨테이너를 배치하고 일을 시킴
    * 워커노드 내부에는 kube-proxy도 있는데, 컨테이너 내부 통신이나 워커노트 사이의 통신을 담당

- 쿠버네티스에서 어플리케이션 실행 flow
    * 개발자가 자기가 필요한 도커 이미지를 이미지 레지스트리에 푸시
    * 앱 디스크립터(yaml or json)으로 어떤 이미지를 어떻게 배치할지, 어떤 특성을 반영해 배치할지를 적은 문서를 작성(이미지가 들어있진 않음, 방법만 적혀있음)
    * 앱 티스크립터를 컨트롤 플레인에게 넘기면, 컨트롤 플레인이 적합한 사용자인지 판단해서 이를 워커노드에 넘김
    * 컨트롤 플레인이 각 워커노드에 적절하게 분산하면, 각 워커 노드들이 레지스트리에서 각 도커 이미지를 받아서 컨테이너 구성후 실행


# 큐브 시스템 컴포넌트
- 큐브 API서버
    - 다른 컨트롤플레인들한테 요청 받아서 워커노드에 있는 큐블렛 통해 워커노드를 제어
    - 컨트롤플레인안의 요소끼리는 통신이 불가능. 오직 큐브 API를 통해서만
    - REST API를 통해서 상태제어
    - 인증, 권한, 승인제어, 리소스 검증및 영구저장 등
- 컨트롤러 매니저
        - API는 아무것도 하지 않아서, 실제 컨트롤은 여기
        - API에 의해 받아진 요청을 처리하는 역할
- 큐브 스케줄러
        - 우리가 보통 실행할 노드를 직접 정해주는것이 아니기 때문에, 스케줄러가 결정해줌
        - 요청받은 리소스를 어느 노드에 실행할지
        - 다수의 팟을 배치하는것은 라운드 로빈으로
    - 큐브 API나, 다른애들도 다 팟의 형태로 떠있다
- ETCD
    - 다중의 키, 벨류 형태의 데이터 저장소
    - Kube api server에 etcd 클라이언트가 들어있음
- POD
    - 쿠버네티스의 빌딩 블록
    - 내부에 다수의 컨테이너를 포함해서 실행할 수 있다
    - 다른 노드에 걸쳐서 실행되지는 않고 항상 하나의 노드에서만 실행
    - 밀접하게 연관된 프로세스를 하나의 환경에서 실행하는것처럼 가능 하지만, 격리되어있음
    - 같은 호스트 / 네임스페이스/ 네트워크 인터페이스를 공유하므로 주의
    - 쿠버네티스 클러스터의 모든 팟은 공유된 단일 플랫, 네트워크 주소 공간에 위치
    - 포드 사이에는 NAT 게이트웨이가 존재하지 않음
    - 외부로 서비스 하기 위해서는 외부에 서비스를 만들어서 해야함
    - 포드 하나에 컨테이너 하나, 컨테이너 하나 안에는 프로세스 하나만 있는게 일반적
        - 붙여서 하는 경우는 뭔가 밀접한 실행이 필요한 환경에서만~
        - 리소스 관찰, 사이드카, 로그 로테이팅등 아니며 그렇게 안함~
        - 한 팟에서 컨테이너끼리 통신은 어차피 같은 IP니 포트를 다르게 뚫어서 localhost로 통신~


# Liveness, Readiness, Startup Probe
- Liveness Probe
    - 컨테이너가 살아있는지 판단하고 다시 시작
    - 컨테이너의 상태를 스스로 판단해서, 교착상태인 컨테이너를 재시작
    - 이거 때문에 버그가 생겨도 높은 가용성이 보장
- Readness Probe
    - Pod가 준비된 상태에 있는지 확인하고 정상 서비스를 시작하는 기능
    - Pod가 준비가 안되 있다면, 로드밸런서에서 해당 Pod로 트래픽을 보내지 않음
- Startup Probe
    - 애플리케이션의 시작 시기를 확인해서 가용성을 높임
    - Liveness와 Readiness의 기능을 비활성화, 컨테이너의 시작 시간을 벌어주는 기능
    - 컨테이너 시작할때 Liveness랑 Readiness가 계속 요청하면 오래걸림
    - 컨테이너가 시작하고 살아있다라는게 Startup에 의해 확인되면 그때 Liveness와 Readiness를 실행
- 결국 모두 Pod을 보조하는 역할이고, Pot의 설정 안에 넣을 수 있음
- Liveness는 확인해가지고 응답이 정상적이지 않으면 팟을 다시 시작하는것
- Readiness도 Liveness와 거의 비슷
- StartUp은 StartUp이 확인하는동안에 Liveness랑 Readiness가 동작 안함, 설정한 시간동안 포드가 정상적으로 동작하지 않으면 종료됨


# 레이블과 설렉터
- 돌아가는 전반에 레이블과 설렉터가 있어야지 실행이 가능함
- 레이블
    - 모든 리소스를 구성하는 쿠버네티스 기능
    - 리소스를 인식하기 위해서 라벨을 붙이는거임
    - 리소스에 부여하는 키/값 쌍임
    - 리소스는 한개 이상의 레이블을 가질 수 있음(다수도 가능)
    - 모든 사람이 쉽게 이해 할 수 있도록 체계적인 시스템을 구축 가능
        - App: 어플리케이션 구성 요소, 마이크로 서비스 유형 지정
        - Rel: 어플리케이션 버전 지정 등

- 레이블 설렉터를 사용하면 각종 리소스를 필터링하여 선택 할 수 있음
- 메타데이터쪽에 레이블을 키와 값 형식으로 저장
- kubectl label 명령으로 추가하기도 가능하고 --overwrite 옵션으로 덮어쓰기가눙, 삭제는 삭제할 레이블 키이름 옆에 - 하면댐



# 레플리케이션 컨트롤러, 레플리카셋, 디플로이먼트
- 레플리케이션 컨트롤러
    - 레플리카셋의 구버전 ( 1.8버전 이전 )
    - 포드가 잘 생성되도록 감시하고, 그 유형의 포드가 원하는 수 만큼 떠있는지 감시해서 그 숫자를 유지  하게 해줌, 문제가 발생하면 새로 만들어줌
    - 팟에 장애가 발생하면 5분정도 후에 인지가 되서, 새로운 팟들을 다른 노드에 만들어줌
        - 쿠버네티스 기본 설정이 5분이고, 변경은 가능
        - 5분 기다리는 이유는, 트래픽 증가로인해 잠시동안 끊어지는것일수도 있기 때문
    - 포드의 범위를 결정하는 레이블 설렉터
    - 실행해야 하는 포드 수를 결정하는 복제본 수
    - 새로운 포드를 설명하는 포드 템플릿
    - 수동, 자동으로 수평 스케일링 됨
    - 노드에 문제가 발생시 다른 노드에 생성해줌
    - 템플릿 부분은 팟 작성하는 방법과 동일
- 쿠버네티스 1.8부터 디플로이먼드, 데몬셋, 레플리카셋, 스테이트풀셋이 추가됨
    - 레플리카셋이 레플리케이션 컨트롤러를 대체함
- 레플리케이션 컨트롤러 vs 레플리카셋
    - 레플리케이션 컨트롤러: 특정 레이블을 포함하는 포드가 있는지 없는지만 볼 수 있음
    - 레플리카셋: 특정레이블이 없거나 해당 값과 관계 없이 특정 레이블 키를 포함하는 포드가 있는지 확인
    - 둘의 구성 요소가 거의 비슷하지만 레플리카셋은 matchExpression이라고 해서 레이블을 매칭하는 별도의 표현 방식이 존재
    - 디플로이먼트 밑에는 레플리카셋이 만들어짐
- 디플로이먼트
    - 다수의 레플리카셋을 관리할수있음
    - 어플리케이션을 다운타임 없이 업데이트 가능하도록 도와주는 리소스
    - 레플리카셋의 상위에 배포되는 리소스
        - 디플로이먼트 -> 레플리카셋 -> 포드 의 순서
    - 모든 포드를 업데이트 하는방법
        - 모든 포드를 부수고 다시 만듬(Re Create) -> 잠깐의 다운타임 발생
        - 롤링 업데이트, 포드를 하나씩 업데이트
    - 스케일링도 간단 
    - matchLabeles에 매칭이 안되면 fail임 주의
    - apply -f 할때 내용이 같으면 업데이트를 안해버리는듯, 주의

# 어플리케이션 롤링 업데이트와 롤백
- 롤링 업데이트 덕분에 다운타임 없이 변경이 가능
- 새 버전을 실행하는동안 서비스가 구 버전의 포드와 연결 -> 서비스의 레이블 셀렉터를 수정하여 간단하게 새 버전으로 바꿀 수 있음
- 로드밸런서가 팟의 레이블을 보고 설렉트를 하기 때문에, v1팟을 실행하면서 v2팟도 실행이 가능함
    - 하지만 하위 호환이 지원은 가능 해야함
- 1.8버전 이전에서는 kubectl을 사용해서 스케일링을 사용해 수동으로 롤링업데이트 했음
    - 레플리케이션 컨트롤러를 2개 이상 돌리면서 수동으로 하던것
    - 사람이 하는거기때문에 실수가 있을수 있으니까 이제 이런거는 하지 말자~
- 이제는 디플로이먼트를 위해 롤링 업데이트가 가능함
- 업데이트 전략은 롤링이 기본이고, 옵션으로 Re Create가 가능하긴 함
- UnDo, ReDo 가능하긴 한데 History에서 사라지는게 아니라 History내에서 위치가 바뀌고 그럼
- 업데이트 관련
    - maxSurge: 최대 몇개까지 팟을 만들것인지
    - maxUnavailable: 최소 몇개까지는 팟을 운영할것인지
- 업데이트 실패 케이스
    - 부족한 할당량
    - 레디네스 프로브 실패
    - 이미지 가져오기 실패
    - 권한 부족
    - 제한 범위(공간마다 할당 가능한 자원의 양을 넘을경우)
    - 응용프로그램 런타임 구성오류
- 업데이트를 실패할경우 기본적으로 600초 후에 업데이트 중지
    - processDeadlineSecond 부분을 수정하면 값 변경 가능
- record=true 옵션 줘야지 기록이 잘되니까 조오심
- 같은 명령을 반복하면 새 history가 찍히는게 아니라 그냥 history가 업데이트됨
    - 기존의 레플리카셋이 남아있기 때문
    - 기존의 레플리카셋은 그냥 size를 0개로 만들어버림(지우진 않음)


# 네임스페이스
- 리소스를 각각의 분리된 영역으로 나누는 방법
- 여러 네임스페이스를 사용하면 복잡한 쿠버네티스 시스템을 더 작은 그룹으로 분할
- 멀티 테넌트 환경을 분리하여 리소스를 개발, QA환경등으로 사용
- 같은 클러스터라도 격리된 환경처럼 활용 가능
- 리소스 이름은 네임스페이스 내에서만 고유함
- get을 옵션 없이 사용하면 default 네임스페이스를 씀
    - —namespace, -n 을 사용해서 네임스페이스별로 확인 가능
- yaml이나 kubectl create namespace 명령어로 간단하게 만들기 가능
- 전체 네임스페이스 조회: kubectl get pod —all-namespace
- 기본 네임스페이스 수정
    - kubectl config set-context --current --namespace=default
    - kubens / kubectx extendstion 사용

# 서비스
- 포드는 문제가 있다
    - 포드는 일시적으로 생성한 컨테이너의 집합
    - 때문에 포드가 지속적으로 생겨났을때 서비스를 하기에 적합하지 않음
    - 쿠버네티스 내부에 있기 때문에, 외부와 통신이 안됨
    - IP가 계속 바뀌고, 로드밸런싱을 관리해줄 또다른 객체가 필요
        - 서비스가 이러한 역할을 해줌
- 서비스의 요구사항
    - 외부 클라이언트가 몇개든 프론트엔드 포드로 연결되어야함
    - 프론트엔드 포드는 백엔드로 연결되어야 하는데, 포드의 ip가 변경되더라도 재설정을 하지 않아야함
    - 로드밸런싱도 서비스가 해줌
- expose가 가장 쉬운 방법
- 기본으로 만들면 ClusterIP로 만들어지는데, 이 IP를 통해서 내부에서 통신
    - 외부로는 노출이 되지않음. 내부에서 쓰는 용도임
- 서비스의 세션을 고정할일도 필요함
    - 서비스가 다수의 포드로 구성하면 웹서비스의 세션이 유지되지 않음
    - 이를 위해서 처음 들어왔던 클라이언트 IP를 그대로유지해주는 방법이 필요
        - sessionAffinity: ClientIP 라는 옵션을 주면 됨
- 다중포트 서비스는 그냥 포트를 여러번 쓰면됨 ㅎㅎ
- 서비스를 describe하면 어떤 엔드포인트가 있는지 알 수 있음
- 외부 IP랑도 연결이 가능함
    - 서비스와 엔드포인트를 모두 만들어줘야함


# 서비스를 외부로 노출하는 방식
- NodePort: 노드 자체포트를 사용해서 포드로 리다이렉션( 30000 ~ 32767 까지만 사용가능)
    - nodePort로 설정. nodePort로 설정된 포트로 요청이 들어오면 큐브프록시가 그걸 봐가지고 적절한 포트를 던져줌 
- LoadBalancer: 외부 게이트웨이를 사용해 노드 포트로 리다이렉션 -> 보통은 클라우드 환경에서 제공해줌 ㅎ(L4, 결국 노드 포트가 제공 되야함)
- Ingress: 하나의IP주소를 통해 여러 서비스를 제공하는 매커니즘(L7, 도메인 이름 해석해서 서브도메인이나 도메인 주소가 다르면 그런걸 탐지해서 어디로 서비스를 나눠줄지를 정해줌, 얘도 노드 포트가 제공 되야함)

# 노드포트
- 3가지를 정해야함, 서비스의 포트, 포드의 포트, 최종적으로 서비스 되는 포트
    - 최종적으로 서비스 되는 포트는 범위가 정해져있음(30000 ~ 32767 )
    - 외부에서 3xxxx 포트로 요청을 하면, svc역할을 하는 kube-proxy가 이를 받아서 적절한 pod에 전달
    - pod이 같은 노드에 없다고 하더라도 kube-proxy가 전달해줌
- 클러스터 관점에서 보면 노드의 포트, 서비스의 포트, 포드의 포트로 나뉘는것임
- 노드포트의 포트가 맘에 들지 않다고 해서 바꾸진 않아도 됨. 로드밸런서에서 바꿔줄수있으니까
- 이걸 이제 노드포트만 해두면 외부에서 접속할려면 노드의 포트를 통해서 직접 접근해야함. 그래서 보통 로드밸런서를 두는것임

# 로드밸런서
- node port 서비스의 확장된 서비스
- 클라우드에서만 활용 가능
- 로드밸런서의 ip주소를 통해 서비스에 엑세스
- 구현하고싶으면 external dns같은거 구현하고 해야되서 복잡

# 인그레스 
- 하나의 IP나 도메인으로 다수의 서비스를 제공
- L7레이어의 로드밸런서로 생각해도 좋을듯
- HTTP요청의 HOST부분을 검사해서, 규칙에 따라서 다른 서비스에 연결 해주는것임
- 도메인 네임은 같아도 디렉토리가 다르면 다른 서비스에 매핑시킬수있음
- 일단 노드 포트는 필요하고, 인그레스를 만들어서 쓰는것
- 물리서버에서 하고싶으면 nginx ingress가 있으니 이걸 참조해볼것
- 인그레스를 통해서 HTTPS도 서비스 가능
    - 보통 웹 방화벽같은게 있으면 앞단의 proxy부분에 인증서를 추가함
    - 그래서 인그레스 서비스에도 TLS인증서 만들수있음

# 네트워크
- 4가지 네트워크 모델
    - 한 포드 내의 다수의 컨테이너끼리 통신
    - 포드 끼리 통신
    - 포드와 서비스 사이의 통신
    - 외부 클라이언트와 서비스 사이의 통신
- 한 포드 내의 다수의 컨테이너끼리 통신
    - Pause 명령어를 통해 아무런 동작을 하지 않는 빈 컨테이너를 생성
    - 일반적인 도커 네트워크랑은 다름, 도커는 컨테이너마다 인터페이스가 별도로 있음
    - 인터페이스를 공유(pause를 통해 만들어진 네트워크를 공유)
    - 포트를 겹치게 구성하지 못하게 하는것이 특징
    - docker의 기능을 사용해 쿠버네티스 컨테이너를 관찰
    - 각 포드마다 하나의 pause 이미지 실행
        - pause는 아무것도 하지 않음. 공유 네트워크를 구성하고 유지하기 위해서 존재하는것(다른 컨테이너들은 언제든지 구성되고 파괴될수있기에)
    - 하나의 팟에는 하나의 인터페이스임
- 포드 끼리의 통신
    - CNI플러그인 필요
        - ACI,AOS,AWS VPN CNI,GCE, Weave Net등 다양한게 있음
        - 클라우드에서 제공해주는것도 있고 직접 깔아서 쓰는것도 있음
        - Weave Net이 많이쓰이는듯함
    - 각 CNI플러그인별 구현 방법과 구조와 기능이 다달라서 이건좀
- 포드와 서비스 사이의 통신
    - ClusterIP를 생성하면 iptables의 설정이 적용됨
    - Kube-proxy가 서비스 트래픽을 제어
    - iptables가 netfilter를 이용해 트래픽 제어(2계층에서 7계층까지 다 커버가능!)
    - 서비스의 ip는 pod에는 없음. 그래서 기본 게이트웨이를 통해서 보내는데, 그렇게 노드의 물리 인터페이스에 접근해서 netfilter를 만나는 순간 목적지를 알게 되고(cluster ip를 하면 netfilter를 쓸 수 있게 되므로), 그 목적지를 kube-proxy와의 통신을 통해서 어딘지 알아냄
- 외부 클라이언트와 서비스 사이의 통신
    - 외부 클라이언트의 요청은 로드밸런서를 거쳐서 들어옴
    - eth0에 들어가게 되면 어느 ip로 보내줄지 결정해야 하는데, 이때 netfilter를 통해 알아낼수있음

# Core DNS
- 서비스를 생성하면 대응하는 DNS엔트리가 생성
    - ```<서비스 이름>.<네임스페이스 이름>.svc.cluster.local``` 의 형식
- 내부에서 DNS 서버 역할을 하는 Pod이 존재 -> Core DNS / Kube DNS
    - kubeapi를 통해서 서비스이름, ip등을 받아와서 저장
    - 각 미들웨어를 통해 로깅, 캐시, 쿠버네티싀 질의를 함
    - Kubernetes 정보, etcd 정보 등 다양한 정보를 들고 있음
- Configmap 저장소를 사용해 설정 파일을 컨트롤
- corefile을 통해 현재 클러스터의 네임스페이스를 지정
- pod에서도 subdomain을 사용하면 dns 서비스를 사용 가능
    - ```<hostname>.<subdomain>.<namespace>.svc.cluster-domain.example``` 의 형식
    - 서브도메인의 이름과 서비스의 이름이 일치 해야함
    - 호스트 이름은 다르게 가능

# Storage
- 볼륨
    - 컨테이너가 외부 스토리지에 엑세스하고 공유하는 방법
    - 포드의 각 컨테이너에는 고유의 분리된 파일 시스템이 존재
    - 볼륨은 포드의 컴포넌트이며 포드의 스펙에 의해 정의
    - 독립적인 쿠버네티스 오브젝트가 아니라서 스스로 생성, 삭제 불가
    - 각 컨테이너의 파일시스템을 볼륨마운트 해서 생성
    - 종류
        - 임시볼륨: emptyDir -> 하나의 포드 안에서 컨테이너 끼리 공유용
        - 로컬볼륨: hostpath, local -> 노드의 파일시스템에 있는 특정 디렉토리임, 데이터를 노드와 컨테이너사이에서 공유하기 위해서, 다른 노드의 포드끼리 데이터 공유는 안댐~, 모니터링용으로 많이씀. 로그 수집하는 도구에
        - 네트워크 볼륨: NFS, iSCSI, gclusterFS -> 클러스터 외부의 자원과 데이터를 공유하기 위해
        - 클라우드 종속적인 네트워크 볼륨: gcpPersistentDisk, awsEBS, azureFile 등
- Gce 영구 스토리지
    - 데이터가 지속
    - 모든 포드에서 공유
    - Mongo db컨테이너에 마운트해서 사용
    - gce에서는 같은 리전에 몽고디비를 만들어둬야함. 안그러면 못찾음
- 기존 네트워크 드라이브의 문제점
    - 개발자가 인프라까지 알아야함. 이는 데브옵스 철학에 맞지 않음
    - 애플리케이션을 배포하는 개발자가 스토리지 기술 종류를 몰라도 상관 없도록 해야함. 인프라 관련 처리는 클러스터 관리자가 해야지 좋다~
    - 이를 해결하려 나온 추상화가 pv(Persistent Volume) <- 제공(관리자가 해줌), pvc(Persistent Volume Claim) <- 요청(개발자가 함)
- PV와 PVC
    - 관리자가 스토리지 연결을 세팅(GCE 스토리지등 )
    - 관리자가 API에 PV를 사용(디스크에 대한 물리적 내용은 PV에만 쓰임)
    - 사용자가 PVC를 생성 ( 사용자는 PV가 어떻게 되있는지 몰라도됨. PVC만 신경쓰면됨)
    - 적절한 크기와 엑세스가 가능한 PVC를 찾고 PV에 바인딩( 권한체크, 용량체크 등)
    - 사용자가 PVC를 참조 하도록 포드 생성
    - 개발자가 pv, pvc를 쓰지 않는다면 물리적 디스크에대한 정보까지 알아야함
    - PVC 작성 요령
        - PVC는 네임스페이스에 속하지 않음
        - 이름으로 매핑되는게 아니라 조건으로 매핑됨
        - 그래도 볼륨이 팟 내부로 들어가야 하기 때문에 이름은 꼭 필요
    - PV 작성요령
        - 용량, Access Mode등이 일치해야함
        - 디스크 정책 정함(삭제시 유지, 삭제시 같이 삭제, 삭제해도 재활용)
        - 이런 설정은 PV생성 후애도 재설정이 가능
        - 여기에 물리적인 내용이 들어가야함(뭐 GCE Persistent Disk등에 대한 정보 같은)
    - 개발자는 Pod와 PVC를 만들고(Pod 내부에서는 PVC를 사용하겟다고 명시), 그 PVC와 매핑되는 PV가 있다면 자동으로 매핑이 됨
    - PV 동적 프로비저닝
        - PV를 직접 만드는대신, 사용자가 원하는 PV유형을 선택하도록 오브젝트 정의
        - Storage Class를 사용하여 PV가 없어도 동적으로 만들어줌
        - 관리자가 Stroage Class만 관리 해주면 됨 
        - 가상환경을 제공해주는 플랫폼을 써야함(GCP, Open Stack 등)
        - Stroage Class 에 프로비저닝에 사용할 플러그인, 매개변수등등을 지정해서 던지면 만들어줌
        - PVC 만들때 strogageClassName부분에 적절한 stroage-class를 넣어서 해주면 그 stroage class를 이용해서 PV가 만들어짐!
        - 로컬에서도 됨 https://github.com/rancher/local-path-provisioner
        - 스토리지 클래스도 디폴트를 해둘수있다
- 스테이트 풀셋
    - 어플리케이션 상태를 저장하고 관리하는데 사용하는 쿠버네티스 객체
        - 포드는 삭제를 하면 안의 내용이 사라지기 때문에 상태를 유지시키기 위해서는 스테이트 풀셋 필요
    - 사용 케이스
        - 안정적이고 고유한 네트워크 식별자가 필요한경우
        - 안정적이고 지속적인 스토리지를 사용해야 하는경우
        - 질서정연한 포드의 배치와 확장을 원하는경우
        - 포드의 자동 롤링업데이트를 사용하기를 원하는 경우
    - 문제점
        - 스테이트 풀셋과 관련된 볼륨이 삭제되지 않아서 관리가 필요 하다
        - 포드의 스토리지는 PV나 스토리지 클래스로 프로비저닝 해야함
        - 롤링 업데이트를 수행하는 경우 수동으로 복구해야할 수 있음(롤링 업데이트 중에 기존의 스토리지와 충돌 - 버전이 다름 등 - 로 인해 오류가 발생 할 수 있다)
        - 포드 네트워크 ID를 유지하기 위해 Headless 서비스가 있어야함 - 고정된 도메인 주소를 제공하기 위함
    - 헤들레스 서비스? -> 기존 서비스와 거의 동일하지만 클러스터IP 부분을 None으로
        - Kube proxy가 밸런싱이나 프록시 형태로 동작하지 않음
        - 각각 서비스의 도메인 네임으로 접근
    - 스테이트풀셋은 기본적으로 디플로이먼트와 거의 유사하지만 좀 다른부분이 있다
        - serviceName: 서비스 네임을 통해 연결하고자 하는 HeadLess 서비스 연결
        - terminationGracePeriodSeconds: 종료요청이후 얼마나 있다가 꺼져야 하는지
        - volumeClaimTemplates: PVC 템플릿. 볼륨이 반드시 들어가야지 유지가 가능함으로
        - volumeMounts: 영구 스토리지 적용 위치
    - 스테이트풀셋은 레플리카를 통해서 다수의 포드가 생성되는데, 만들어진 순서대로 0~n까지 이름이 부여된다
    - pv, pvc가 하나씩 할당되어서, 걔를 죽이고 나중에 다시 실행해도 pv와 pvc는 남아있어서 원래걸로 연결됨
    - 배포순서는 순차고, 종료나 업데이트 순서는 역순임
    - 업데이트 전략은 디플로이먼트랑 똑같이 한번에 삭제하고 하거나(OnDelete), 하나씩 완료하고 하는 롤링이 있음
        - 하나가 완전히 업뎃되서 실행이 되어야지 다음게 실행이 됨

# 애플리케이션 스케줄링과 라이프사이클 관리

- 어플리케이션 환경 변수 관리
    - 쿠버네티스의 환경변수는 YAML파일이나 다른 리소스로 전달 가능
    - 하드코딩(yaml에 써진)된 환경변수는 여러 환경에 데이터를 정의하거나 유지,관리 하기가 어려움
        - 일괄적으로 변경하려면 모든 yaml파일을 수정해야 하기 때문
        - 이런 문제를 해결하는 방법이 ConfigMap, Secret임
    - 도커 컨테이너에서의 환경 설정
        - 환경변수
            - env:  name / value
        - ConfigMap
            - env: name / valueFrom / configMapKeyRef
        - Secrets
            - env: name / valueFrom / secretKeyRef
    - ConfigMap
        - 환경변수 저장 뿐만아니라 파일 저장도 가능함
        - Kubectl 명령어를 통해 바로 저장 할 수 있음
            - 파일을 통해서 만드는 방법이 가장 보편적
        - 컨피그맵에는 이름이 존재하고, 각 데이터별로 key-value 형태로 저장되있어서, 사용할때는 사용할 컨피그맵이름을 지정하고, 사용할 키 이름을 적어주면 끝
        - 키-벨류 모두 가져오려면, 키를 지정하지 않고 valueFrom 하면 됨
        - 볼륨에 마운트 하는법(CoreDNS가 하는 방법이 이런식임)
            - volumes에 comfigMap을 넣으면 됨
        - 컨테이너를 재시작하지않아도 1분마다 Refresh가 되어서 변경한게 바로 적용됨
        - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables
    - Secret
        - ConfigMap과 유사. 인코딩 해서 저장하는 정도의 차이
        - 비밀번호, OAuth토큰, ssh키와 같은 민감한 정보 저장용
        - 그냥 Base64인코딩이라 완전히 안전한것은 아님
        - 명령어를 통해 저장하면 알아서 Base64인코딩이 되지만, yaml파일을 통해서 만들면 내가 직접 인코딩 해서 저장 해야함
        - Etcd에 저장이 되며, 허가된 팟에서만 접근이 가능
        - 환경변수로 매핑될떄에는 디코딩 되서 들어가기에 configmap하고 큰 차이는 없음
    - 초기 명령어 및 Arg 전달과 실행
        - 초기실행 명령어
            - spec.containers.command와 arg에 원하는 커맨드랑 argument 넣으면 실행됨
            - 환경변수를 활용하여 출력할때는 $를 사용하여 명령 내용 변경 가능
    - 한 포드에 멀티 컨테이너
        - 하나의 포드에 다수의 컨테이너를 사용하는 경우 네트워크 인터페이스, IPC, 볼륨을 공유할 수 있음
        - 보통 모니터링 시스템이나 컨텐츠 프로바이더가 따로있는경우 뭉쳐서 사용 -> 이런 경우를 사이드카 컨테이너라고 부름
        - 개발자가 만든 컨테이너를 포드에서 관찰을 해줘야 하는데, 보통 이런 모니터링용 사이드카 컨테이너를 맨듬
        - 성능상에 조금 부하가 더 걸리기는 하지만, 안전한 시스템을 만들 수 있음
    - Init 컨테이너
        - 다수의 컨테이너를 올리는 방법중 하나
        - 주 컨테이너가 실행되기전에 초기화 하면서 정리 해주는 역할
        - init컨테이너가 끝나야지 주 컨테이너가 실행됨
        - init컨테이너가 실패하면 성공 할때까지 포드를 반복해서 재시작
        - restartPolicy를 never를 주면 재시작하지 않음
        - 주 컨테이너가 다른 컨테이너에 의존성이 있을때 사용하면 조음
    - 시스템 리소스 요구사항과 제한
        - 리소스 = CPU, 메모리 등
        - 리소스 요청은 spec.containers.resource.requests.cpu / memory
        - 리소스 제한은 spec.containers.resource.limits.cpu / memory
        - 디스크 같은건 Persistent Volume같은걸로 많이함(디스크 제한도 가능하긴 함)
        - 리소스 기준
            - CPU: 1 Hyper Thread Core 기준 (m 은 1/1000) -> 환경마다 1개의 의미가 다름(ex> 1 AWS vCPU, 1 GCP Core, 1 Azure vCore등)
            - Memory: Ti, Gi, Ki(1024씩 증가), T, G, M, K(1000씩 증가)
        - 이러한 리소스 제한은 컨테이너 정의 부분에 쓰임
        - 오토스케일링 할때 쓰임
        - GUP도 나중에 들가는데 얘는 찐 GPU 하나임. virtual GPU는 비쌈
        - 리미트 레인지
            - 네임스페이스 / 포드 / 컨테이너 별로 리소스를 제한하는 정책
            - 네임스페이스 / 포드 / 컨테이너 당 최소 및 최대, default 리소스 제한
            - 네임스페이스에서 PersistentVolumeClaim 당 최소 및 최대 스토리지 사용량 제한
            - 네임스페이스에서 리소스에 대한 요청과 제한 사이의 비율 적용
            - 네임스페이스에서 리소스에 대한 디폴트 requests/limits 를설정하고 런타임중인 컨테이너에 자동으로 입력
            - kubeapi에다가 limiteRange 옵션을 줘야지 쓸수있음(클라우드별 설정에 따라 다름)
            - type을 통해서 네임스페이스, 포드, PVC, 컨테이너를 설정
        - 리소스 쿼타
            - 네임스페이스별 리소스 제한
            - 리미트레인지 같은건 리소스의 정책으로 제한하는거고 리소스 쿼타는 총 량을 제한하는것
            - 네임스페이스 내의 모든 컨테이너의 합이 ~ 를 넘지 않도록 -> 이런식